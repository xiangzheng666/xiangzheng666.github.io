---
categories: [西瓜书]
tags: [决策树]
---
# 决策树

- ## ID3信息增益划分准则

  - 信息熵
  
    - $$
      Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k
      $$
    
  - 信息增益
  
    - $$
    Gain(D,a)=Ent(D)-\sum_{v=1}^{V}{|D^v| \over |D|}Ent(D^v)
      $$
    
  - 对取值较多的属性有偏好，一个属性取值越多，对于 每种取值纯度越高，但泛化能力弱
  
- ## C4.5增益率划分准则

  - 属性的固有值，越大，取值越多

    - $$
      IV(a)=-\sum_{v=1}^V{|D^v| \over |D|}log_2{|D^v| \over |D|}
      $$

  - 增益率

    - $$
      Gain\_ratio(D,a)={Gain(D,a) \over IV(a)}
      $$

      

  - 使用增益率划分会对取值数目较少的属性有偏好

  - 在C4.5中先使用信息增益选出高于平均值的，再用增益率挑选最高的

- ## CART基尼系数划分准则

  - 基尼值，越小纯度越高

    - $$
      Gini(D)=1-\sum_{k=1}^{|y|}p_k^2
      $$

  - 基尼系数

    - $$
      Gini\_index(D,a)=\sum_{v-=1}^{V}{|D^v| \over |D|}Gini(D^v)
      $$

  - 与信息增益都对取值多的属性有偏好

- ## 预剪枝

  - **在决策树生成过程中，对每个结点在划分前进行估计**
  - **主要是根据验证集的性能是否提升进行是否对节点进行划分**
  - **可以提高泛化能力，降低过拟合的风险**

- ## 后剪枝 

  - **先训练一颗完整的树，自底向上对非叶子节点进行比较,使用非叶子节点代替字数，看验证机是否有提升**

- ## 连续值

  - **会对每种值进行二分，计算评价指标，进行节点划分**

- ## 缺失值

	- 对于缺失值，划分使用无缺失值的样本进行计算
	- 对于判断缺失值的样本时，使用权值乘以对应节点的非空值所占比例，选择概率最大的结点

- ## 多变量决策树

  - 单变量决策树，每个节点都是对一个属性进行判断，在数学上看他的边界是一个与坐标轴平行的线
  - 在多变量中，节点使用多个属性的线性组合进行判断，在数学上看他的边界是一个与坐标轴相交的线

